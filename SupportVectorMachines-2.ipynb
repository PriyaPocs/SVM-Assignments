{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6eb052-4e8e-4558-b3b1-990af9b5cef5",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "\n",
    "Answer(Q1):\n",
    "\n",
    "Polynomial functions and kernel functions are related concepts in machine learning, particularly in the context of support vector machines (SVMs) and other kernel-based methods.\n",
    "\n",
    "1. **Polynomial Functions:**\n",
    "Polynomial functions are mathematical expressions that involve variables raised to integer powers, multiplied by coefficients, and summed together. In the context of machine learning, polynomial functions are often used to transform input data into a higher-dimensional space. This can be useful when the original data is not linearly separable but becomes separable in a higher-dimensional space. For example, in a 2D space, a polynomial transformation might involve adding features like x^2, y^2, xy, etc., to make the data linearly separable in the transformed space.\n",
    "\n",
    "2. **Kernel Functions:**\n",
    "Kernel functions are used in various machine learning algorithms, notably SVMs, to implicitly map input data into a higher-dimensional space without explicitly computing the transformation. They provide a computationally efficient way to perform this transformation, which is especially useful when the transformation would lead to a high computational cost. The kernel trick, as it's called, allows you to work in the original input space while still benefiting from the effects of a higher-dimensional transformation.\n",
    "\n",
    "The most commonly used kernel function is the Radial Basis Function (RBF) kernel, also known as the Gaussian kernel. It measures the similarity between data points in the transformed space. Other kernel functions, such as polynomial kernels, are also used to implicitly apply polynomial transformations.\n",
    "\n",
    "3. **Relationship:**\n",
    "Polynomial functions can be seen as a specific type of kernel function. In fact, a polynomial kernel is used to implicitly apply a polynomial transformation to the data. Instead of explicitly calculating the transformed features, the polynomial kernel computes the pairwise similarity between data points in the original space using a polynomial expression. This way, the effect of applying a polynomial transformation is achieved without the need to explicitly compute the transformed features.\n",
    "\n",
    "In summary, polynomial functions and kernel functions are related through the idea of applying transformations to input data to make it more amenable to classification or regression tasks. Polynomial kernels are a specific type of kernel function that implicitly applies polynomial transformations, offering computational efficiency through the kernel trick."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698ec9b-2c9a-4905-920f-04c3ab5081e1",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "\n",
    "Answer(Q2):\n",
    "\n",
    "You can implement an SVM with a polynomial kernel using the Scikit-learn library in Python. Here's a step-by-step guide on how to do that:\n",
    "\n",
    "1. **Install Scikit-learn:**\n",
    "\n",
    "2. **Import Required Libraries:**\n",
    "   Import the necessary libraries including Scikit-learn's SVM module and any other libraries you might need.\n",
    "\n",
    "3. **Generate or Load Data:**\n",
    "   Generate synthetic data or load your dataset. In this example, we'll generate a random classification dataset.\n",
    "\n",
    "4. **Split Data into Training and Testing Sets:**\n",
    "   Split your data into training and testing sets.\n",
    "   \n",
    "5. **Create SVM with Polynomial Kernel:**\n",
    "   Create an SVM classifier with a polynomial kernel using the `SVC` class from Scikit-learn. Specify the `kernel` parameter as `'poly'` and set other relevant parameters like `degree` for the degree of the polynomial.\n",
    "\n",
    "6. **Train the SVM:**\n",
    "   Train the SVM on the training data.\n",
    "\n",
    "7. **Make Predictions:**\n",
    "   Use the trained SVM to make predictions on the test data.\n",
    "\n",
    "\n",
    "8. **Evaluate the Model:**\n",
    "   Evaluate the performance of the SVM by calculating accuracy or other relevant metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958e79b6-4307-4ad3-bf18-0a0e67514768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create SVM with polynomial kernel\n",
    "svm_classifier = svm.SVC(kernel='poly', degree=3)\n",
    "\n",
    "# Train the SVM\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d7ea1-69cd-4e38-a991-d4d632908bbf",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "\n",
    "Answer(Q3):\n",
    "\n",
    "In Support Vector Regression (SVR), the parameter \"epsilon\" (often denoted as ε) defines the width of the ε-insensitive tube around the regression line within which errors are ignored. In other words, data points falling within this tube are not considered errors and do not contribute to the loss function used for training the SVR model. Data points outside this tube are considered errors and contribute to the loss.\n",
    "\n",
    "The ε-insensitive tube is important because SVR aims to find a regression line that fits the data within this tube while minimizing the error for data points outside the tube. The width of the tube is controlled by the epsilon parameter.\n",
    "\n",
    "Now, let's discuss how increasing the value of epsilon affects the number of support vectors in SVR:\n",
    "\n",
    "1. **Smaller Epsilon:**\n",
    "   - A smaller epsilon leads to a narrower ε-insensitive tube.\n",
    "   - As the tube becomes narrower, the SVR model tries to fit the data more closely, potentially allowing more data points to fall outside the tube.\n",
    "   - This can result in more support vectors, as points that fall outside the narrow tube become support vectors that influence the placement of the regression line.\n",
    "\n",
    "2. **Larger Epsilon:**\n",
    "   - A larger epsilon results in a wider ε-insensitive tube.\n",
    "   - With a wider tube, the SVR model allows more data points to fall within the tube without considering them as errors.\n",
    "   - As a result, fewer data points are treated as support vectors, because the model can achieve a good fit without relying on them.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR generally leads to a decrease in the number of support vectors, while decreasing the value of epsilon tends to increase the number of support vectors. The choice of epsilon depends on the trade-off between fitting the data closely and maintaining a simpler model. A larger epsilon allows for a more forgiving fit and fewer support vectors, which can lead to a smoother and more generalized model, while a smaller epsilon can result in a more precise but potentially more complex model with more support vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc9713-46e7-4ce3-8b6f-d7850630ef58",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "\n",
    "Answer(Q4):\n",
    "\n",
    "Support Vector Regression (SVR) is a powerful regression technique that uses support vector machines to perform regression tasks. The performance of SVR can be influenced by several parameters: the choice of kernel function, the C parameter, the epsilon parameter, and the gamma parameter. Let's discuss how each parameter works and how they affect SVR's performance:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - The kernel function determines how the input data is transformed into a higher-dimensional space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - **Effect:** The choice of kernel influences the flexibility and complexity of the model. Some data might be better separated or modeled with certain kernels.\n",
    "   - **Example:** Use a polynomial kernel when you suspect a polynomial relationship between features and the target. Use an RBF kernel for complex relationships that aren't linear.\n",
    "\n",
    "2. **C Parameter (Regularization):**\n",
    "   - The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A smaller C emphasizes a larger margin with potential misclassified points, while a larger C allows for fewer misclassified points but a smaller margin.\n",
    "   - **Effect:** Smaller C values create a smoother regression function, potentially ignoring some noisy data points. Larger C values focus more on fitting the data precisely, even if it means accepting more deviations.\n",
    "   - **Example:** Increase C when the data is believed to have minimal noise and overfitting is a concern. Decrease C to create a more robust model that generalizes better.\n",
    "\n",
    "3. **Epsilon Parameter (Tube Width):**\n",
    "   - The epsilon parameter determines the width of the ε-insensitive tube around the regression line. Data points within the tube do not contribute to the loss function.\n",
    "   - **Effect:** A smaller epsilon results in a tighter tube, leading to more support vectors and potentially overfitting. A larger epsilon results in a wider tube and fewer support vectors.\n",
    "   - **Example:** If the data has inherent noise or uncertainty, increase epsilon to allow more points within the tube. If you want a precise fit, decrease epsilon.\n",
    "\n",
    "4. **Gamma Parameter (RBF Kernel Specific):**\n",
    "   - The gamma parameter defines the influence of a single training example. A small gamma means the influence is 'far,' while a large gamma means the influence is 'close.'\n",
    "   - **Effect:** A smaller gamma leads to a smoother and more generalized model, while a larger gamma makes the model focus more on individual data points.\n",
    "   - **Example:** For an RBF kernel, increase gamma to make the model fit the training data more closely, but be cautious of overfitting. Decrease gamma for a smoother and more general fit.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "- Choose the appropriate kernel based on the type of relationship you expect in your data.\n",
    "- Adjust the C parameter to control the balance between fitting noise and fitting the training data.\n",
    "- Modify the epsilon parameter to define the tolerance for errors within the ε-insensitive tube.\n",
    "- Tweak the gamma parameter (if using an RBF kernel) to control the impact of individual data points on the model.\n",
    "\n",
    "It's important to note that parameter tuning often involves experimentation and cross-validation to find the optimal values for your specific dataset and problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cbff3-9085-44d5-81cc-02977e4623ab",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "\n",
    "Import the necessary libraries and load the dataset\n",
    "\n",
    "Split the dataset into training and testing set\n",
    "\n",
    "Preprocess the data using any technique of your choice(e.g.scaling,normalization)\n",
    "\n",
    "Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "Use the trained classifier to predict the labels of the testing data\n",
    "\n",
    "Evaluate the performance of the classifier using any metric of your choice(e.g.accuracy, precision, recall, F1-score)\n",
    "\n",
    "Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance\n",
    "\n",
    "Train the tuned classifier on the entire dataset\n",
    "\n",
    "Save the trained classifier to a file for future use.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f499e95-0a84-40af-a35a-c5111c5b36aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svc_classifier.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "import joblib\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(X)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing: Scaling the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creating an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Training the classifier on the scaled training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Using the trained classifier to predict labels for testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating the performance using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_svc_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Training the tuned classifier on the entire dataset\n",
    "X_scaled = scaler.transform(X)\n",
    "best_svc_classifier.fit(X_scaled, y)\n",
    "\n",
    "# Saving the trained classifier to a file\n",
    "joblib.dump(best_svc_classifier, 'tuned_svc_classifier.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
